{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIC Week 5 - Papers With Code\n",
    "## Attention is All You Need\n",
    "Link to paper: https://arxiv.org/pdf/1706.03762.pdf <br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with importing needed librarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the all the libraries installed, run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup some parameters for our 'model', and generate a random sequence to play the part of embeddings for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "seq_len = 8\n",
    "d_model = 8\n",
    "# Pretend these are embeddings of something like words\n",
    "# Each column is a word embedding\n",
    "sequence = np.random.rand(embedding_dim, seq_len)\n",
    "for row in sequence:\n",
    "    for element in row:\n",
    "        print(round(element, 3), end=', ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinusoidal Positional Embeddings\n",
    "Positional embedding should match the length of the regular embedding\n",
    "For each index in our embeddings, the positional embedding should be calculated as a sine or cosine (alternating starting with sin), and the frequency should be altered based on the equation given in the paper.<br/>\n",
    "\n",
    "The below cell will show a vizualization of this although the formula is not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretend seq_len is 8 for vizualization purposes\n",
    "x = np.linspace(0, 8, 100)\n",
    "# Use one indexing so the first position doesn't just become one\n",
    "for i in range(1, 8+1):\n",
    "    if i % 2 == 0:\n",
    "        y = np.cos(i*x*math.pi/(8*2))\n",
    "        plt.plot(x, y)\n",
    "    else:\n",
    "        y = np.sin(i*x*math.pi/(8*2))\n",
    "        plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can can generate the positional embeddings for a given sequence length as seen below (implemented as a tensorflow layer).<br/>\n",
    "In Attention Is All You Need, they scale their waves based on the following formulas:\n",
    "$$\n",
    "PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "Where $pos$ is the position in the sequence, $i$ is the dimension, and $d_{model}$ is the size of the vectors of the input tokens, output tokens, and thus the positional encodings as well.<br/><br/>\n",
    "For more clarity, $pos$ determines where along the sinusoid we should get the value, and $i$ helps determine what the frequency of that sinusoid should be; The same correlations can be seen in the graph above and implementation below.<br/><br/>\n",
    "They scale it in this way because they add the positional embedding with the original token embeddings and they found that to be a good balance between not having positions tower over learned token embeddings and vice versa.<br/>\n",
    "Another proposed solution is to concatenate the learned embeddings to the positional ones, however this takes more memory and computational power.<br/><br/>\n",
    "The idea behind the changes in frequency is that the low frequencies are good for capturing long term dependancies and the high frequencies are good for finding specific, exact locations (ie, moving a certain distance along a wave with a higher frequency has a bigger change in value). And with a higher embedding space you can get more granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        poses = np.tile(np.arange(inputs.shape[1])+1, (inputs.shape[0], 1))\n",
    "        dims = np.tile(np.arange(inputs.shape[0])+1, (inputs.shape[1], 1)).T\n",
    "        pos_embeddings = np.zeros(poses.shape)\n",
    "        pos_embeddings[::2] = np.sin(poses[::2]/(10000**(2*dims[::2]/self.d_model)))\n",
    "        pos_embeddings[1::2] = np.cos(poses[1::2]/(10000**(2*dims[1::2]/self.d_model)))\n",
    "        return inputs + pos_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "Now that the input has it's embeddings, we can put everything through an attention mechanism.<br/>\n",
    "The first step is to map the input embeddings to queries, keys, and values. The queries, keys, and values are learned as linear/dense/fully-connected layers from the input embeddings.<br/><br/>\n",
    "The q, k, and v weights are unbiased as well.<br/>\n",
    "\n",
    "The Queries, Keys, and Values are matrices of the same shape as the input. We represent them as $Q, K, V$<br/>\n",
    "They call their attention mechanism Scaled Dot-Product Attention and define it as show below where $d_{k}$ is the dimension of the queries and keys. (They also define the dimension of values as $d_{v}$ however it is usually the same as $d_{k}$)\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt(d_{k})})V\n",
    "$$\n",
    "They argue that the reason they use the scaling is because for large $d_{k}$ values, the dot product gets so large that the softmax has very small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.q_w = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "\n",
    "        )\n",
    "        self.k_w = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.v_w = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, q=None, k=None, v=None):\n",
    "        if not q:\n",
    "            q = tf.matmul(inputs, self.q_w)\n",
    "        if not k:\n",
    "            k = tf.matmul(inputs, self.k_w)\n",
    "        if not v:\n",
    "            v = tf.matmul(inputs, self.v_w)\n",
    "        kq = tf.matmul(k, q, transpose_b=True) / math.sqrt(q.shape[1])\n",
    "        o = tf.matmul(tf.nn.softmax(kq), v)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output can be seen below, the numbers are gibberish but we can see that the shapes all fit what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = SinusoidalEmbedding(d_model)\n",
    "x = embed_layer(sequence)\n",
    "print(x)\n",
    "att_layer = Attention()\n",
    "x = att_layer(sequence)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention\n",
    "Multi-head attention is the same thing as single head attention, but done multiple times in parallel (multiple k,q,v's) but then at the end the outputs are all concatenated together and put into a FC layer that outputs the same shape as the inputs (and thus can be stacked)<br/><br/>\n",
    "$$\n",
    "MultiHead(Q, K, V) = Concat(head_{1}, ..., head_{h})W^{O}\n",
    "$$\n",
    "$$\n",
    "head_{i} = Attention(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})\n",
    "$$\n",
    "The idea behind multiple heads is that each head can attend to a different place (ie, The dog eats steak, when looking at dog we might want one head to attend to eats, and another head to attend to steak)<br/><br/>\n",
    "They also suggest dividing $d_{model}$ by the number of heads so the size of the network size and computation cost is relatively the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            self.heads.append(Attention())\n",
    "        self.output_layer = tf.keras.layers.Dense(input_shape[-1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        os = []\n",
    "        for head in self.heads:\n",
    "            os.append(head(inputs))\n",
    "        return self.output_layer(tf.concat(os, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output can be seen below, the numbers are gibberish but we can see that the shapes all fit what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = SinusoidalEmbedding(d_model)\n",
    "x = embed_layer(sequence)\n",
    "mha = MultiHeadAttention(4)\n",
    "x = mha(x)\n",
    "x = mha(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "## Easy - 1 pt\n",
    "  - As we discussed in this paper, most models add position embeddings to the regular embeddings. However, there are some that concatenate embeddings instead. This works better but has much worse space complexity. Can you create a new embedding layer that concatenates the embeddings instead of adding them.\n",
    "## Medium - 2 pt\n",
    "  - Create a pointwise feedforward network as described in the paper.\n",
    "## Hard - 3 pt\n",
    "  - Create the encoder and decoder as described in the paper.\n",
    "    - (If you did the medium problem, you have everything you need, just smush it all together correctly)\n",
    "## Super Hard - 6 pt\n",
    "  - Implement the rest of a transformer as described in the paper and train it on some data (things like translation is good)\n",
    "    - You'll need to add batch dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b239acf2821489c398a9848859e84ce39b99d30cc4031fb37cc7461da3883639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
